\FloatBarrier
\section{Summary of Results}
The experiment in~\Cref{Chapters/Experiments-and-Results/Null-Hypothesis-1} has rejected the \textit{Null Hypothesis 1} (\Cref{Chapters/Introduction/Null-Hypothesis-1}). Therefore, by using the statistical analysis, given that we know there is an error in the implementation, we can detect a significant difference in the behaviour in setup $B$ when compared to the benchmark implementation in setup $A$. This is a positive result, as it confirms that the initial verification of the simulator and its goals were achieved.

On the other hand, the experiment in~\Cref{Chapters/Experiments-and-Results/Null-Hypothesis-2} failed to reject the \textit{Null Hypothesis 2} (\Cref{Chapters/Introduction/Null-Hypothesis-2}). This means that the difference in behaviour between setup $A$ and setup $B$ was not statistically significant. We have performed a number experiments for this hypothesis, varying the parameters like the length of the simulation or the number of targets. Increasing the length of the simulations seemed to improve the results, but not enough to account for a statistically significant difference. We think the explanation for this result is that the effect is more subtle than this particular experimental setup is able to detect. This might be due to the fact that the simulations are still relatively short, therefore the difference in the frequency of trading between the two setups does not have a detectable effect; however, if the simulations were run for longer, the effect could be more significant. The only trend that we have been able to observe is that as the algorithm trades more often, the errors have much less variance. Hence, we think that using a different metric for this particular hypothesis would be beneficial.
